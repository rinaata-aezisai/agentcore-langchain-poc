#!/usr/bin/env python3
"""AgentCore vs LangChain ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æœ¬æ ¼çš„ãªæ¯”è¼ƒæ¤œè¨¼ã‚’å®Ÿè¡Œã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚
å„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ç‰¹å¾´ã‚’æ´»ã‹ã—ãŸå®Ÿè¡Œã‚’è¡Œã„ã€
å®¢è¦³çš„ãªæ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã€‚
"""

import argparse
import asyncio
import json
import os
import sys
import time
from dataclasses import asdict, dataclass, field
from datetime import datetime, UTC
from pathlib import Path
from typing import Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "backend" / "src"))
sys.path.insert(0, str(project_root / "poc" / "strands-agents"))
sys.path.insert(0, str(project_root / "poc" / "langchain"))


@dataclass
class TestCase:
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©"""
    id: str
    name: str
    prompt: str
    category: str
    use_tools: bool = False
    expected_tool: str | None = None
    description: str = ""


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""
    test_id: str
    test_name: str
    category: str
    strands_latency_ms: int
    langchain_latency_ms: int
    strands_success: bool
    langchain_success: bool
    strands_response: str = ""
    langchain_response: str = ""
    strands_tool_calls: int = 0
    langchain_tool_calls: int = 0
    strands_memory_size: int = 0
    langchain_memory_size: int = 0
    strands_features: list[str] = field(default_factory=list)
    langchain_features: list[str] = field(default_factory=list)
    latency_diff_ms: int = 0
    faster_framework: str = ""
    timestamp: str = ""


@dataclass
class BenchmarkSummary:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é›†è¨ˆçµæœ"""
    total_tests: int
    strands_wins: int
    langchain_wins: int
    strands_avg_latency_ms: float
    langchain_avg_latency_ms: float
    strands_success_rate: float
    langchain_success_rate: float
    strands_total_tool_calls: int
    langchain_total_tool_calls: int
    by_category: dict[str, dict[str, Any]] = field(default_factory=dict)


# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
TEST_CASES: list[TestCase] = [
    # åŸºæœ¬å¿œç­”ãƒ†ã‚¹ãƒˆ
    TestCase(
        id="basic-greeting",
        name="åŸºæœ¬æŒ¨æ‹¶",
        prompt="ã“ã‚“ã«ã¡ã¯ï¼è‡ªå·±ç´¹ä»‹ã‚’ã—ã¦ãã ã•ã„ã€‚",
        category="basic",
        description="ã‚·ãƒ³ãƒ—ãƒ«ãªå¿œç­”ãƒ†ã‚¹ãƒˆ",
    ),
    TestCase(
        id="basic-qa",
        name="åŸºæœ¬Q&A",
        prompt="Pythonã®ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ã«ã¤ã„ã¦ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        category="basic",
        description="çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®å¿œç­”ãƒ†ã‚¹ãƒˆ",
    ),

    # ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆ
    TestCase(
        id="memory-context",
        name="ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿æŒ",
        prompt="ç§ã®åå‰ã¯ç”°ä¸­å¤ªéƒã§ã™ã€‚è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ã€‚",
        category="memory",
        description="ãƒ¡ãƒ¢ãƒªæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ",
    ),
    TestCase(
        id="memory-recall",
        name="ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ³èµ·",
        prompt="ç§ã®åå‰ã‚’è¦šãˆã¦ã„ã¾ã™ã‹ï¼Ÿ",
        category="memory",
        description="ãƒ¡ãƒ¢ãƒªã‹ã‚‰ã®æƒ³èµ·ãƒ†ã‚¹ãƒˆ",
    ),

    # ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ãƒ†ã‚¹ãƒˆ
    TestCase(
        id="tool-weather",
        name="å¤©æ°—å–å¾—",
        prompt="æ±äº¬ã®ä»Šæ—¥ã®å¤©æ°—ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        category="tool_use",
        use_tools=True,
        expected_tool="get_current_weather",
        description="å¤©æ°—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ",
    ),
    TestCase(
        id="tool-calculate",
        name="è¨ˆç®—",
        prompt="123 * 456 + 789 ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚",
        category="tool_use",
        use_tools=True,
        expected_tool="calculate",
        description="è¨ˆç®—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ",
    ),
    TestCase(
        id="tool-search",
        name="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢",
        prompt="AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¤ã„ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚",
        category="tool_use",
        use_tools=True,
        expected_tool="search_documents",
        description="æ¤œç´¢ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ",
    ),
    TestCase(
        id="tool-multi",
        name="è¤‡æ•°ãƒ„ãƒ¼ãƒ«",
        prompt="ã‚¿ã‚¹ã‚¯ã€Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨ˆç”»ã®ä½œæˆã€ã‚’ä½œæˆã—ã€ãã®å¾Œãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã®å¤©æ°—ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        category="multi_tool",
        use_tools=True,
        description="è¤‡æ•°ãƒ„ãƒ¼ãƒ«é€£ç¶šå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ",
    ),

    # é•·æ–‡ãƒ†ã‚¹ãƒˆ
    TestCase(
        id="long-response",
        name="é•·æ–‡ç”Ÿæˆ",
        prompt="AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœªæ¥ã«ã¤ã„ã¦ã€500æ–‡å­—ç¨‹åº¦ã®ã‚¨ãƒƒã‚»ã‚¤ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
        category="long_form",
        description="é•·æ–‡ç”Ÿæˆèƒ½åŠ›ãƒ†ã‚¹ãƒˆ",
    ),

    # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
    TestCase(
        id="error-handling",
        name="ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°",
        prompt="å­˜åœ¨ã—ãªã„éƒ½å¸‚ã€Œãƒ ãƒ¼ãƒ³ã‚·ãƒ†ã‚£ã€ã®å¤©æ°—ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        category="error",
        use_tools=True,
        description="ä¸æ­£å…¥åŠ›æ™‚ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°",
    ),
]


class Benchmark:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self, iterations: int = 3):
        self.iterations = iterations
        self.results: list[BenchmarkResult] = []
        self._strands_adapter = None
        self._langchain_adapter = None

    def _get_strands_adapter(self):
        """Strandsã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å–å¾—"""
        if self._strands_adapter is None:
            from strands_poc.adapter import create_strands_adapter
            self._strands_adapter = create_strands_adapter()
        return self._strands_adapter

    def _get_langchain_adapter(self):
        """LangChainã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å–å¾—"""
        if self._langchain_adapter is None:
            from langchain_poc.adapter import create_langchain_adapter
            self._langchain_adapter = create_langchain_adapter()
        return self._langchain_adapter

    async def run_strands_test(self, test_case: TestCase) -> dict[str, Any]:
        """Strands Agentsã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        adapter = self._get_strands_adapter()
        start_time = time.time()

        try:
            if test_case.use_tools:
                response = await adapter.execute_with_tools(
                    context=[],
                    instruction=test_case.prompt,
                )
            else:
                response = await adapter.execute(
                    context=[],
                    instruction=test_case.prompt,
                )

            latency_ms = int((time.time() - start_time) * 1000)

            return {
                "success": True,
                "latency_ms": latency_ms,
                "content": response.content[:500] if response.content else "",
                "tool_calls": len(response.tool_calls) if response.tool_calls else 0,
                "memory_size": response.metadata.get("memory_size", 0) if response.metadata else 0,
                "features": response.metadata.get("framework_features", []) if response.metadata else [],
                "error": None,
            }
        except Exception as e:
            return {
                "success": False,
                "latency_ms": int((time.time() - start_time) * 1000),
                "content": "",
                "tool_calls": 0,
                "memory_size": 0,
                "features": [],
                "error": str(e),
            }

    async def run_langchain_test(self, test_case: TestCase) -> dict[str, Any]:
        """LangChainã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        adapter = self._get_langchain_adapter()
        start_time = time.time()

        try:
            if test_case.use_tools:
                response = await adapter.execute_with_tools(
                    context=[],
                    instruction=test_case.prompt,
                )
            else:
                response = await adapter.execute(
                    context=[],
                    instruction=test_case.prompt,
                )

            latency_ms = int((time.time() - start_time) * 1000)

            return {
                "success": True,
                "latency_ms": latency_ms,
                "content": response.content[:500] if response.content else "",
                "tool_calls": len(response.tool_calls) if response.tool_calls else 0,
                "memory_size": response.metadata.get("memory_size", 0) if response.metadata else 0,
                "features": response.metadata.get("framework_features", []) if response.metadata else [],
                "error": None,
            }
        except Exception as e:
            return {
                "success": False,
                "latency_ms": int((time.time() - start_time) * 1000),
                "content": "",
                "tool_calls": 0,
                "memory_size": 0,
                "features": [],
                "error": str(e),
            }

    async def run_test_case(self, test_case: TestCase) -> BenchmarkResult:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ"""
        print(f"  Running: {test_case.name} ({test_case.category})")

        # è¤‡æ•°å›å®Ÿè¡Œã—ã¦å¹³å‡ã‚’å–ã‚‹
        strands_latencies = []
        langchain_latencies = []
        strands_result = None
        langchain_result = None

        for i in range(self.iterations):
            # Strandså®Ÿè¡Œ
            strands_result = await self.run_strands_test(test_case)
            strands_latencies.append(strands_result["latency_ms"])

            # LangChainå®Ÿè¡Œ
            langchain_result = await self.run_langchain_test(test_case)
            langchain_latencies.append(langchain_result["latency_ms"])

        # å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨ˆç®—
        strands_avg = int(sum(strands_latencies) / len(strands_latencies))
        langchain_avg = int(sum(langchain_latencies) / len(langchain_latencies))

        # çµæœã‚’æ§‹ç¯‰
        latency_diff = strands_avg - langchain_avg
        faster = "strands" if strands_avg < langchain_avg else "langchain"

        return BenchmarkResult(
            test_id=test_case.id,
            test_name=test_case.name,
            category=test_case.category,
            strands_latency_ms=strands_avg,
            langchain_latency_ms=langchain_avg,
            strands_success=strands_result["success"] if strands_result else False,
            langchain_success=langchain_result["success"] if langchain_result else False,
            strands_response=strands_result["content"] if strands_result else "",
            langchain_response=langchain_result["content"] if langchain_result else "",
            strands_tool_calls=strands_result["tool_calls"] if strands_result else 0,
            langchain_tool_calls=langchain_result["tool_calls"] if langchain_result else 0,
            strands_memory_size=strands_result["memory_size"] if strands_result else 0,
            langchain_memory_size=langchain_result["memory_size"] if langchain_result else 0,
            strands_features=strands_result["features"] if strands_result else [],
            langchain_features=langchain_result["features"] if langchain_result else [],
            latency_diff_ms=latency_diff,
            faster_framework=faster,
            timestamp=datetime.now(UTC).isoformat(),
        )

    async def run_all(self) -> BenchmarkSummary:
        """å…¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ"""
        print(f"Starting benchmark with {len(TEST_CASES)} test cases, {self.iterations} iterations each")
        print("=" * 60)

        for test_case in TEST_CASES:
            result = await self.run_test_case(test_case)
            self.results.append(result)

            # çµæœã‚’è¡¨ç¤º
            status = "âœ“" if result.strands_success and result.langchain_success else "âœ—"
            print(f"    {status} Strands: {result.strands_latency_ms}ms, LangChain: {result.langchain_latency_ms}ms")
            print(f"      â†’ Faster: {result.faster_framework} (diff: {abs(result.latency_diff_ms)}ms)")

        print("=" * 60)
        return self._calculate_summary()

    def _calculate_summary(self) -> BenchmarkSummary:
        """é›†è¨ˆçµæœã‚’è¨ˆç®—"""
        total = len(self.results)
        strands_wins = sum(1 for r in self.results if r.faster_framework == "strands")
        langchain_wins = total - strands_wins

        strands_latencies = [r.strands_latency_ms for r in self.results if r.strands_success]
        langchain_latencies = [r.langchain_latency_ms for r in self.results if r.langchain_success]

        strands_avg = sum(strands_latencies) / len(strands_latencies) if strands_latencies else 0
        langchain_avg = sum(langchain_latencies) / len(langchain_latencies) if langchain_latencies else 0

        strands_success_rate = sum(1 for r in self.results if r.strands_success) / total * 100
        langchain_success_rate = sum(1 for r in self.results if r.langchain_success) / total * 100

        strands_tool_calls = sum(r.strands_tool_calls for r in self.results)
        langchain_tool_calls = sum(r.langchain_tool_calls for r in self.results)

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
        by_category: dict[str, dict[str, Any]] = {}
        for result in self.results:
            cat = result.category
            if cat not in by_category:
                by_category[cat] = {
                    "count": 0,
                    "strands_wins": 0,
                    "strands_avg_latency": [],
                    "langchain_avg_latency": [],
                }
            by_category[cat]["count"] += 1
            if result.faster_framework == "strands":
                by_category[cat]["strands_wins"] += 1
            by_category[cat]["strands_avg_latency"].append(result.strands_latency_ms)
            by_category[cat]["langchain_avg_latency"].append(result.langchain_latency_ms)

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥å¹³å‡ã‚’è¨ˆç®—
        for cat, data in by_category.items():
            data["strands_avg_latency"] = (
                sum(data["strands_avg_latency"]) / len(data["strands_avg_latency"])
                if data["strands_avg_latency"] else 0
            )
            data["langchain_avg_latency"] = (
                sum(data["langchain_avg_latency"]) / len(data["langchain_avg_latency"])
                if data["langchain_avg_latency"] else 0
            )

        return BenchmarkSummary(
            total_tests=total,
            strands_wins=strands_wins,
            langchain_wins=langchain_wins,
            strands_avg_latency_ms=strands_avg,
            langchain_avg_latency_ms=langchain_avg,
            strands_success_rate=strands_success_rate,
            langchain_success_rate=langchain_success_rate,
            strands_total_tool_calls=strands_tool_calls,
            langchain_total_tool_calls=langchain_tool_calls,
            by_category=by_category,
        )

    def save_results(self, output_dir: Path) -> None:
        """çµæœã‚’ä¿å­˜"""
        output_dir.mkdir(parents=True, exist_ok=True)

        # è©³ç´°çµæœ
        results_path = output_dir / "benchmark-results.json"
        with open(results_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "generated_at": datetime.now(UTC).isoformat(),
                    "iterations": self.iterations,
                    "results": [asdict(r) for r in self.results],
                    "summary": asdict(self._calculate_summary()),
                },
                f,
                indent=2,
                ensure_ascii=False,
            )
        print(f"Results saved to: {results_path}")

        # Markdown ãƒ¬ãƒãƒ¼ãƒˆ
        report_path = output_dir / "benchmark-report.md"
        self._generate_markdown_report(report_path)
        print(f"Report saved to: {report_path}")

    def _generate_markdown_report(self, path: Path) -> None:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        summary = self._calculate_summary()

        report = f"""# AgentCore vs LangChain ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

**ç”Ÿæˆæ—¥æ™‚**: {datetime.now(UTC).strftime("%Y-%m-%d %H:%M:%S UTC")}
**åå¾©å›æ•°**: {self.iterations}

## ã‚µãƒãƒªãƒ¼

| é …ç›® | Strands Agents | LangChain |
|------|----------------|-----------|
| å‹åˆ©æ•° | {summary.strands_wins} | {summary.langchain_wins} |
| å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | {summary.strands_avg_latency_ms:.0f}ms | {summary.langchain_avg_latency_ms:.0f}ms |
| æˆåŠŸç‡ | {summary.strands_success_rate:.1f}% | {summary.langchain_success_rate:.1f}% |
| ç·ãƒ„ãƒ¼ãƒ«å‘¼å‡º | {summary.strands_total_tool_calls} | {summary.langchain_total_tool_calls} |

## ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ

| ã‚«ãƒ†ã‚´ãƒª | ãƒ†ã‚¹ãƒˆæ•° | Strandså‹åˆ© | Strandså¹³å‡ | LangChainå¹³å‡ |
|----------|----------|-------------|-------------|---------------|
"""

        for cat, data in summary.by_category.items():
            report += f"| {cat} | {data['count']} | {data['strands_wins']} | "
            report += f"{data['strands_avg_latency']:.0f}ms | {data['langchain_avg_latency']:.0f}ms |\n"

        report += """
## è©³ç´°çµæœ

| ãƒ†ã‚¹ãƒˆå | ã‚«ãƒ†ã‚´ãƒª | Strands | LangChain | å‹è€… | å·®åˆ† |
|----------|----------|---------|-----------|------|------|
"""

        for r in self.results:
            strands_status = "âœ“" if r.strands_success else "âœ—"
            langchain_status = "âœ“" if r.langchain_success else "âœ—"
            winner = "ğŸ”µ" if r.faster_framework == "strands" else "ğŸŸ£"

            report += f"| {r.test_name} | {r.category} | "
            report += f"{strands_status} {r.strands_latency_ms}ms | "
            report += f"{langchain_status} {r.langchain_latency_ms}ms | "
            report += f"{winner} {r.faster_framework} | {abs(r.latency_diff_ms)}ms |\n"

        report += """
## ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ©Ÿèƒ½æ¯”è¼ƒ

### Strands Agents (AgentCore)
"""
        # æœ€åˆã®çµæœã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’å–å¾—
        if self.results and self.results[0].strands_features:
            for feature in self.results[0].strands_features:
                report += f"- {feature}\n"

        report += """
### LangChain + LangGraph
"""
        if self.results and self.results[0].langchain_features:
            for feature in self.results[0].langchain_features:
                report += f"- {feature}\n"

        report += """
---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ `scripts/benchmark.py` ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
"""

        with open(path, "w", encoding="utf-8") as f:
            f.write(report)


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="AgentCore vs LangChain ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"
    )
    parser.add_argument(
        "--iterations", "-i",
        type=int,
        default=3,
        help="å„ãƒ†ã‚¹ãƒˆã®åå¾©å›æ•° (default: 3)"
    )
    parser.add_argument(
        "--output", "-o",
        type=str,
        default="docs/reports",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: docs/reports)"
    )
    args = parser.parse_args()

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    benchmark = Benchmark(iterations=args.iterations)

    try:
        summary = await benchmark.run_all()

        # çµæœã‚’è¡¨ç¤º
        print("\nğŸ“Š Summary:")
        print(f"  Total tests: {summary.total_tests}")
        print(f"  Strands wins: {summary.strands_wins}")
        print(f"  LangChain wins: {summary.langchain_wins}")
        print(f"  Strands avg latency: {summary.strands_avg_latency_ms:.0f}ms")
        print(f"  LangChain avg latency: {summary.langchain_avg_latency_ms:.0f}ms")

        # çµæœã‚’ä¿å­˜
        output_dir = Path(args.output)
        benchmark.save_results(output_dir)

        print("\nâœ… Benchmark completed!")

    except Exception as e:
        print(f"\nâŒ Benchmark failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
